# Twincare \U0001F491

*A privacyâ€‘first virtual healthcare assistant that performs symptom triage, appointment booking, and insurance guidance â€” all powered by onâ€‘prem LLMs and FastAPI.*

---

## âœ¨ Key Features

| Feature                     | Description                                                                                                                                                     |
| --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Multiâ€‘agent routing**     | MCPCore intelligently routes each user message to specialised agents (medical triage, booking, insurance) using semantic similarity + zeroâ€‘shot classification. |
| **Onâ€‘device LLM inference** | Uses a local **Llama.cpp** Medâ€‘Alpaca model for HIPAAâ€‘friendly, lowâ€‘latency language generation.                                                                |
| **Context persistence**     | Conversation state is cached in Redis with TTLâ€‘controlled keys so agents retain shortâ€‘term memory without longâ€‘term PII retention.                              |
| **FastAPI microservice**    | Thin HTTP layer exposes a clean JSON API (REST) with automatic OpenAPI docs.                                                                                    |
| **Plugâ€‘andâ€‘play agents**    | Â Agents share a common `BaseChatAgent` interface, making it trivial to add new verticals (nutrition, mental health, etc.).                                      |
| **Observability hooks**     | Core metrics (request latency, error count, token usage) emitted via `/stats` endpoint and ready for Prometheus / Grafana.                                      |

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TD
    subgraph Client
        A[Web / Mobile App]
    end

    A -->|HTTPS JSON| B[FastAPI Router]
    B --> C[MCPCore \n(message orchestrator)]
    C -->|conversation id| D[Redis]
    C --> E[MedicalChatAgent]
    C --> F[BookingAgent]
    C --> G[InsuranceAgent]

    %% LLM calls
    subgraph Onâ€‘DeviceÂ LLMs
        E --> H[Llama.cpp \nMedâ€‘Alpacaâ€‘2â€‘7B]
        E --> I[HF Zeroâ€‘Shot \nBARTâ€‘MNLI]
    end

    %% External integrations
    F --> J[CalendarÂ API]
    G --> K[InsuranceÂ API]
```

*Highâ€‘level message flow:*

1. **Client** sends `POST /route`.
2. **FastAPI Router** validates payload (`AgentRequest`).
3. **MCPCore** checks Redis for context âœ chooses best agent.
4. Selected **Agent** generates a response (may call LLM / external APIs).
5. Response is returned as `AgentResponse` â†’ client UI.

### MCPCore (Message Control Plane)

MCPCore is the **central routing brain** of TwinCare. It performs three critical tasks before delegating work to downstream agents:

1. **Intent inference & scoring**
   Utilises a Sentenceâ€‘Transformer (`allâ€‘MiniLMâ€‘L6`) and a zeroâ€‘shot NLI classifier to compute a confidence score for every registered agent. The highestâ€‘scoring agent wins; ties are broken by a deterministic roundâ€‘robin.
2. **Context stitching**
   Pulls the userâ€™s shortâ€‘term context from Redis, merges it with the current prompt, and injects a conversation header that preserves PHI boundaries.
3. **Safety gatekeeping**
   Runs each outgoing prompt through a lightweight Medâ€‘ToxiScore model to block disallowed content before the LLM fires.

Because MCPCore is stateless **by design**, you can scale it horizontally behind any load balancer without sticky sessions.

---

## ğŸ“ Repository Layout

```text
app/
â”œâ”€â”€ agents/            # MedicalChatAgent, BookingAgent, ...
â”œâ”€â”€ context/           # Redisâ€‘backed ContextManager
â”œâ”€â”€ core/              # MCPCore orchestrator
â”œâ”€â”€ config/            # settings.py, redis_config.py
â”œâ”€â”€ protocol/          # Pydantic schemas + API router
â”œâ”€â”€ utils/             # (e.g., encryption stubs)
â”œâ”€â”€ main.py            # FastAPI startup
tests/                 # PyTest suites
requirements.txt       # Python deps
Dockerfile (coming soon)
```

---

## ğŸš€ GettingÂ Started

### Prerequisites

* PythonÂ 3.9+
* RedisÂ 5+
* GCCÂ / clang (for llama.cpp build)
* (Optional) GPU w/ CUDAÂ 11+ for HF pipelines

### 1Â Â·Â Clone & create virtualenv

```bash
git clone https://github.com/hubHarshit/twincare.git
cd twincare-backend
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### 2Â Â·Â Download the LLM weights

```bash
# example â€” adjust path and model as needed
wget https://huggingface.co/medalpaca/MedAlpaca-2-7B-GGUF/resolve/main/med-alpaca-2-7b-chat.Q4_K_M.gguf \
     -O models/Med-Alpaca-2-7b-chat.Q4_K_M.gguf
```

### 3Â Â·Â Configure environment

Create a **.env** file (or export vars)Â â€” values below are defaults.

```env
# .env
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_TTL=86400          # seconds
MODEL_PATH=models/Med-Alpaca-2-7b-chat.Q4_K_M.gguf
```

### 4Â Â·Â Run the API

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Visit **[http://localhost:8000/docs](http://localhost:8000/docs)** for interactive Swagger UI.

---

## ğŸ—ºï¸ API Reference

### `POST /route`

| Field        | Type   | Description                |
| ------------ | ------ | -------------------------- |
| `user_id`    | string | Unique ID for user session |
| `input_text` | string | Userâ€™s message             |
| `context`    | object | (Optional) extra metadata  |

<details>
<summary>SampleÂ cURL</summary>

```bash
curl -X POST http://localhost:8000/route \
     -H "Content-Type: application/json" \
     -d '{
           "user_id": "abc123",
           "input_text": "I have a sore throat and fever",
           "context": {}
         }'
```

</details>

### `GET /stats`

Returns JSON with request count, average latency, error tally, etc.

### `GET /agent/{agent_id}/status`

Health information for a particular agent.

---

## ğŸ§ª RunningÂ Tests

```bash
pytest -q
```

---

## ğŸ“¦ Docker (optional)

> **Coming soon** â€” multiâ€‘stage build with llama.cpp + poetry.

```Dockerfile
# skeleton
FROM python:3.9-slim AS base
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY app/ app/
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## âš™ï¸ Deployment

TwinCare can run everywhereâ€”from a single GPU laptop to a fully managed, HIPAAâ€‘compliant stack on **GoogleÂ Cloud Platform**. Below are the most common footprints.

### Local (Docker Compose)

```bash
docker compose -f ops/docker/docker-compose.local.yml up --build -d
```

* Spins up **API** + **Redis** containers.
* Binds the `models/` host folder so you can iterate on quantised `*.gguf` weights without rebuilding the image.

### Kubernetes on GKE

* Helm chart lives in `helm/twincare`.
* Creates:

  * **Deployment** for FastAPI pods (`replicas=3`, HPA on CPU + queue length)
  * **RedisCluster** via redis-operator
  * **PVC** (ReadWriteOnce) to store llama.cpp weights
  * **ConfigMap** with environment settings
* Add a GPU nodeâ€¯pool (e.g. n1â€‘standardâ€‘4â€‘T4) and switch the inference backend to `llama.cpp --gpu-layers=35` for 10Ã— throughput.

### Cloud RunÂ +Â VertexÂ AI (serverlessâ€‘GPU)

1. **BuildÂ &Â Push**: CloudÂ Build executes `cloudbuild.yaml` â†’ pushes image to **ArtifactÂ Registry**.
2. **Serve API**: Deploy the revision to **CloudÂ Run** (`--cpu=2 --memory=4Gi --max-instances=40`).
3. **LLM Offload**: Flip the envÂ var `USE_VERTEX_AI=1` and route all `MedicalChatAgent` completions to a **VertexÂ AI** TextÂ Generation Inference endpoint (Gemmaâ€‘7B, PaLMâ€‘2, or custom model).
4. **Async Tasks**: Longâ€‘running EHR or pVerify calls are dispatched to **CloudÂ Tasks**.
5. **Observability**: CloudÂ Run automatically exports logs to CloudÂ Logging; metrics/traces are picked up by CloudÂ Monitoring with an opinionated dashboard JSON under `ops/monitoring`.

```mermaid
graph LR
  subgraph CI/CD
    CB[CloudÂ Build Trigger] --> AR[ArtifactÂ Registry]
    AR --> CR[CloudÂ Run Revision]
  end
  CR --> LB[CloudÂ LoadÂ Balancer]
  CR --> REDIS[(CloudÂ Memorystore)]
  CR -->|Predict| VTX[VertexÂ AI Endpoint]
```

### Secrets & Compliance

* **SecretÂ Manager** holds API tokens; the Helm chart autoâ€‘mounts them as envÂ vars.
* Enable **VPCâ€‘SC** + **PrivateÂ ServiceÂ Connect** for VertexÂ AI if PHI data ever leaves the pod.
* For auditability, MCPCore emits a structured JSON audit log for every decision edgeâ€”ship it to **BigQuery** via LogÂ Router.

---

## ğŸ¤ Contributing

1. Fork the repo & create your branch: `git checkout -b feature/awesome`
2. Run `make precommit` (black, isort, flake8, mypy)
3. Submit a PR â€” describe *why* & *how*.

All discussions happen in **GitHubÂ Issues** âœÂ feel free to open feature ideas or bug reports.

---

## ğŸ“„ License

MIT Â©Â HarshitÂ Pant

---

## ğŸ™ Acknowledgements

* [Medâ€‘Alpaca](https://github.com/medalpaca) and [Llamaâ€‘cpp](https://github.com/ggerganov/llama.cpp)
* [FastAPI](https://fastapi.tiangolo.com/) community
* [SentenceTransformers](https://www.sbert.net/)
* Inspiration from StanfordÂ HAI â€œAI for Healthcareâ€ labs

> *Stay healthy, stay private â€” TwinCare has your back.*
